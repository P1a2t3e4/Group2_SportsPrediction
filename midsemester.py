# -*- coding: utf-8 -*-
"""Midsemester.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1t8wIZP5QixV1Glv_xA3SWFiOQJROUYOh
"""

#Here, we are mounting and loaiing our file
import pandas as pd
import os
from google.colab import drive
drive.mount('/content/drive')
file_path= os.path.abspath(r"/content/drive/MyDrive/players_21.csv")
# Creating a variable with the name "df" that keeps the data
df= pd.read_csv(file_path)
df
#commenting
#forward fill
#add at least two more models-train the ones he mentioned in the rubric
#parameters

# The various columns with urls were removed because we knew they were not needed in getting the overall ratings of players. Their short names,long names and date of birth were removed as well because ideally, they should not influence the rating of players.
remove_columns =['player_url','short_name','long_name','player_face_url','dob','player_face_url','club_logo_url','club_flag_url','nation_logo_url','nation_flag_url']
df=df.drop(columns=remove_columns)
df

# Checking for values that have misssing values greater than 30 %
missing_percentage = (df.isnull().mean() * 100)

# We defined a threshold of 30% for the maximum allowed missing values
threshold = 30

# To get the list of columns with missing values exceeding the threshold
columns_to_drop = missing_percentage[missing_percentage > threshold].index.tolist()

# dropping columns with excessive missing values from the DataFrame
df.drop(columns=columns_to_drop, inplace=True)
df

import pandas as pd


# Identifying the categorical columns
categorical_columns = df.select_dtypes(include=['object']).columns

# Identifying the numeric columns
numeric_columns = df.select_dtypes(exclude=['object']).columns

# Creating separate DataFrames for categorical and numeric data
categorical_data = df[categorical_columns]
numeric_data = df[numeric_columns]

#imputing the numeric and categorical data before the correlation
#Filling missing values in numeric data with the median.
from sklearn.impute import SimpleImputer
imp=SimpleImputer(strategy='median')
a=imp.fit_transform(numeric_data)
numeric_data_impute = pd.DataFrame(a,columns=numeric_columns)

# Filling missing values in categorical_data using forward fill (ffill)
categorical_data_imputed = categorical_data.fillna(method='ffill')

# Making sure the result is a DataFrame with the same column names
categorical_data_imputed = pd.DataFrame(categorical_data_imputed, columns=categorical_data.columns)

#Displaying the categorical_data_imputed and numeric_data_impute
categorical_data_imputed.info()
numeric_data_impute.info()

#Encoding the categorical data
from sklearn.preprocessing import LabelEncoder

# Initializing the LabelEncoder
label_encoder = LabelEncoder()

# Looping through each column in the imputed categorical data
for col in categorical_data_imputed.columns:
    categorical_data_imputed[col] = label_encoder.fit_transform(categorical_data_imputed[col])

import pandas as pd
# Combining the imputed numeric and label-encoded categorical data into a single DataFrame
df = pd.concat([numeric_data_impute, categorical_data_imputed], axis=1)

#df = pd.concat([pd.DataFrame(a, columns=numeric_data.columns), categorical_data_imputed], axis=1)
df.info()

#drop the useless features
#drop all features with more than 30% missing values
#impute the numeric and non-numeric
#encode the non-numeric
#feature selection
#then put them together
#feature selection
#scale
#from scaling, split
#from split, train
#from train, test
#use 22 dataset

# Selecting features in the dataset that most correlate with the target variable,overall
correlations = df.corrwith(df['overall'])

sorted_correlations = correlations.abs().sort_values(ascending=False)
#We chose a threshold of 0.615 in order to ensure we got an appropriate number of selected feautures
threshold = 0.615

#Selecting and storing feature names with correlations above the given threshold(0.615).
selected_features = sorted_correlations[sorted_correlations > threshold].index.tolist()

#printing out the selected features
print("Selected Features:")
print(selected_features)

# Dropping non-selected features from the dataset
non_selected_features = [col for col in df.columns if col not in selected_features]

df.drop(non_selected_features, axis=1, inplace=True)
df

# Assigning the column of overall to the variable y
y=df['overall']

#dropping overall from the dataset
df=df.drop(columns='overall')

#assigning df to x
x=df

#scaling x
from sklearn.preprocessing import StandardScaler

# Creating an instance of StandardScaler
scaler = StandardScaler()

# Fitting the scaler to my x and transforming them
scaled = scaler.fit_transform(x)

#pickling the model #creating the scalar model
import pickle
pickle_out=open("scaled.pkl","wb")
pickle.dump(scaled,pickle_out)
pickle_out.close()

#TSplitting the dataset into training and testing sets, with 20% of the data reserved for testing, using a fixed random seed for reproducibility.
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(scaled, y, test_size=0.2, random_state=42)

#Checking the shape (dimensions) of the variable X_train
X_train.shape

#Checking the shape(dimensions) of the variable X_test
X_test.shape

#Grid search and cross validation
#use the ensemble
#the one for training-split, testing ,pass as it is
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import KFold
import xgboost as xgb

# Using the Random Forest Regressor
rf = RandomForestRegressor()
rf.fit(X_train, y_train)


# Using the gradientboosting regressor
gb = GradientBoostingRegressor(max_depth=2,n_estimators=3, learning_rate=1.0)
gb.fit(X_train, y_train)

#Using the XGBoost
xgb_regression = xgb.XGBRegressor()
xgb_regression.fit(X_train, y_train)

# Finding the mean score for each model to get an initial sense of how each model is performing before hyperparameter tuning.
rf_score= cross_val_score(rf, X_test, y_test, cv=5)
print("The Random Forest Regressor score is :",rf_score.mean())

gb_score= cross_val_score(gb, X_test, y_test, cv=5)
print("The Gradient Boosting Regressor score is :",gb_score.mean())

xgb_regression_score= cross_val_score(rf, X_test, y_test, cv=5)
print("The XGBoost Regressor score is :",xgb_regression_score.mean())

from sklearn.metrics import mean_absolute_error
# Defining the hyperparameters and their possible values for the grid search
param_grid = {
    'n_estimators': [100, 150, 150],
    'max_depth': [5, 5, 10] ,
    #'min_samples_split': [2, 5, 1],  # least samples required to split an internal node
    #'min_samples_leaf': [1, 2, 4]  # minimum samples needed to be a leaf node
     }


# Creating a grid search with cross-validation for the RandomForest
grid_search_rf = GridSearchCV(estimator=rf, param_grid=param_grid, cv=KFold(n_splits=5, shuffle=True, random_state=32))

# Fitting the grid search to the training data
grid_search_rf.fit(X_train, y_train)

# Getting the best model with the best hyperparameters
best_rf = grid_search_rf.best_estimator_

best_rf.fit(X_train, y_train)  # Fit the best model to the training data

# Predict on the test data
y_pred = best_rf.predict(X_test)

# Calculate the Mean Absolute Error (MAE)
mae = mean_absolute_error(y_test, y_pred)
print("Mean Absolute Error for the Best Random Forest Model:", mae)

# Creating a grid search with cross- validation for gradientBoosting
grid_search_gb = GridSearchCV(estimator=gb, param_grid=param_grid, cv=KFold(n_splits=5, shuffle=True, random_state=32))

# Fitting the grid search to the training data
grid_search_gb.fit(X_train, y_train)

# Getting the best model with the best hyperparameters
best_gb = grid_search_gb.best_estimator_

best_gb.fit(X_train, y_train)  # Fit the best model to the training data

# Predict on the test data
y_pred = best_gb.predict(X_test)

# Calculate the Mean Absolute Error (MAE)
mae = mean_absolute_error(y_test, y_pred)
print("Mean Absolute Error for the Best Random Forest Model:", mae)

param = {
    'n_estimators': [100, 200, 300],
    'max_depth': [10, 15, 20] ,
    # 'subsample': [0.5,0.75, 1.0],  # least samples required to split an internal node
    # 'learning rate': [0.1,0.05,0.01]  # minimum samples needed to be a leaf node
    #
    }


# Creating a grid search with cross-validation for the XGBoost
grid_search_xgb = GridSearchCV(estimator=xgb_regression, param_grid=param, cv=KFold(n_splits=5, shuffle=True, random_state=32))

# Fitting the grid search to the training data
grid_search_xgb.fit(X_train, y_train)

# Getting the best model with the best hyperparameters
best_xgb = grid_search_xgb.best_estimator_

best_xgb.fit(X_train, y_train)  # Fit the best model to the training data

# Predict on the test data
y_pred = best_xgb.predict(X_test)

# Calculate the Mean Absolute Error (MAE)
mae = mean_absolute_error(y_test, y_pred)
print("Mean Absolute Error for the Best Random Forest Model:", mae)

from sklearn.ensemble import VotingRegressor
voting_regression = VotingRegressor(estimators=[
    ('RandomForest', best_rf),
    ('GradientBoosting', best_gb),
    ('XGBoost', best_xgb)
])

# Fitting the ensemble to the training data
voting_regression.fit(X_train, y_train)

# Predicting on the test data using the ensemble model
ensemble_predictions = voting_regression.predict(X_test)

# Evaluating the performance of the ensemble model using mean squared error
from sklearn.metrics import mean_squared_error
ensemble_mse = mean_squared_error(y_test, ensemble_predictions)

print("MSE for the Ensemble Model:", ensemble_mse)

from sklearn.metrics import mean_absolute_error

# Evaluating the performance of the ensemble model using mean absolute error
mae = mean_absolute_error(ensemble_predictions, y_test)
print("Mean Absolute Error for Ensemble:", mae)

#mounting the drive
import pandas as pd
import os
from google.colab import drive
drive.mount('/content/drive')

#loading our data
file_path= os.path.abspath(r"/content/drive/MyDrive/players_22.csv")
data= pd.read_csv(file_path)

#assigning the selected features we got from players_21 to x
selected_features_2 = ['overall','movement_reactions', 'mentality_composure', 'passing', 'lcm', 'cm', 'rcm', 'potential', 'lm', 'rm']

x = data[selected_features_2]

#Assigning overall to y
y = data['overall']

import pandas as pd

# Identifying the categorical columns within our selected features
categorical_columns = x.select_dtypes(include=['object']).columns

# Identifying the numeric columns within our selected feautures
numeric_columns = x.select_dtypes(exclude=['object']).columns

# Creating separate DataFrames for categorical and numeric data
categorical_data = x[categorical_columns]
numeric_data = x[numeric_columns]

#imputing the numeric and categorical data before the correlation
#Filling missing values in numeric data with the median.
from sklearn.impute import SimpleImputer
imp=SimpleImputer(strategy='median')
a=imp.fit_transform(numeric_data)
numeric_data_impute = pd.DataFrame(a,columns=numeric_columns)

# Filling missing values in categorical_data using forward fill (ffill)
categorical_data_imputed = categorical_data.fillna(method='ffill')

# Making sure the result is a DataFrame with the same column names
categorical_data_imputed = pd.DataFrame(categorical_data_imputed, columns=categorical_data.columns)

#Encoding the categorical data
from sklearn.preprocessing import LabelEncoder

# Initializing the LabelEncoder
label_encoder = LabelEncoder()

# Looping through each column in the imputed categorical data
for col in categorical_data_imputed.columns:
    categorical_data_imputed[col] = label_encoder.fit_transform(categorical_data_imputed[col])

import pandas as pd
# Combining the imputed numeric and label-encoded categorical data into a single DataFrame
x = pd.concat([numeric_data_impute, categorical_data_imputed], axis=1)

#displaying x
x.info()

#removing overall from the selected feautures
x=x.drop(columns='overall')

#scaling my x
from sklearn.preprocessing import StandardScaler

# Creating an instance of StandardScaler
scaler = StandardScaler()

# Fitting the scaler to our selected features and transforming them
scaled = scaler.fit_transform(x)

from sklearn.metrics import mean_absolute_error
from sklearn.ensemble import RandomForestRegressor

#Using the Random Forest Regressor model to test player22
model = RandomForestRegressor(n_estimators=100, random_state=32)  # Adjust hyperparameters as needed

# Fitting the model to the entire dataset
model.fit(a, y)

# Predicting on the entire dataset
y_pred = model.predict(a)

# Calculating the mean absolute error
mae = mean_absolute_error(y, y_pred)
print("Mean Absolute Error:", mae)

# Testing the GradientBoosting Regressor
from sklearn.ensemble import GradientBoostingRegressor
model = GradientBoostingRegressor(n_estimators=100, random_state=32)  # Adjust hyperparameters as needed

# Fitting the model to the entire dataset
model.fit(a, y)

# Predicting on the entire dataset
y_pred = model.predict(a)

# Calculating the mean absolute error
mae = mean_absolute_error(y, y_pred)
print("Mean Absolute Error:", mae)

# Testing the xgtBoosting Regressor
import xgboost as xgb
model =xgb.XGBRegressor(n_estimators=100, random_state=32)  # Adjust hyperparameters as needed

# Fitting the model to the entire dataset
model.fit(a, y)

# Predicting on the entire dataset
y_pred = model.predict(a)

# Calculating the mean absolute error
mae = mean_absolute_error(y, y_pred)
print("Mean Absolute Error:", mae)

#Saving the model

import pickle
pickle_out=open("bestmodel.pkl","wb")
pickle.dump(voting_regression, pickle_out)
pickle_out.close()