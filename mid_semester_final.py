# -*- coding: utf-8 -*-
"""Mid-semester final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zTKSXf2xfDgY_XtDg7NFYXOFQ33TN8_Y
"""

#Here, we are mounting and loading our file

import pandas as pd
import os
from google.colab import drive
drive.mount('/content/drive')
file_path= os.path.abspath(r"/content/drive/MyDrive/midsem-project/players_21.csv")
# Creating a variable with the name "df" that keeps the data
df= pd.read_csv(file_path)
df

# The various columns with urls were removed because we knew they were not needed in getting the overall ratings of players. Their short names,long names and date of birth were removed as well because ideally, they should not influence the rating of players.
remove_columns =['player_url','short_name','long_name','player_face_url','dob','player_face_url','club_logo_url','club_flag_url','nation_logo_url','nation_flag_url','sofifa_id','club_team_id']
df=df.drop(columns=remove_columns)
df

# Checking for values that have misssing values greater than 30 %
missing_percentage = (df.isnull().mean() * 100)

# We defined a threshold of 30% for the maximum allowed missing values
threshold = 30

# To get the list of columns with missing values exceeding the threshold
columns_to_drop = missing_percentage[missing_percentage > threshold].index.tolist()

# dropping columns with excessive missing values from the DataFrame
df.drop(columns=columns_to_drop, inplace=True)
df

import pandas as pd
# Identifying the categorical columns
categorical_columns = df.select_dtypes(include=['object']).columns

# Identifying the numeric columns
numeric_columns = df.select_dtypes(exclude=['object']).columns

# Creating separate DataFrames for categorical and numeric data
categorical_data = df[categorical_columns]
numeric_data = df[numeric_columns]

#imputing the numeric and categorical data before the correlation
#Filling missing values in numeric data with the median.
from sklearn.impute import SimpleImputer
imp=SimpleImputer(strategy='median')
a=imp.fit_transform(numeric_data)
numeric_data_impute = pd.DataFrame(a,columns=numeric_columns)

# Filling missing values in categorical_data using forward fill (ffill)
categorical_data_imputed = categorical_data.fillna(method='ffill')

# Making sure the result is a DataFrame with the same column names
categorical_data_imputed = pd.DataFrame(categorical_data_imputed, columns=categorical_data.columns)

#Encoding the categorical data
from sklearn.preprocessing import LabelEncoder

# Initializing the LabelEncoder
label_encoder = LabelEncoder()

# Looping through each column in the imputed categorical data
for col in categorical_data_imputed.columns:
    categorical_data_imputed[col] = label_encoder.fit_transform(categorical_data_imputed[col])

import pandas as pd
# Combining the imputed numeric and label-encoded categorical data into a single DataFrame
df = pd.concat([numeric_data_impute, categorical_data_imputed], axis=1)

df.info()

from sklearn.ensemble import RandomForestRegressor
#making use of feauture importance
# Assuming 'X' is my feature matrix and 'y' is my target variable
X = df.drop(['overall'], axis=1)
y = df['overall']

# Initializing a Random Forest Regressor
rf = RandomForestRegressor(random_state=32)

# Fitting the model to my data
rf.fit(X, y)

# Getting feature importances
feature_importances = rf.feature_importances_

# Creating a DataFrame to associate features with their importance scores
feature_importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importances})

# Sorting the features by importance in descending order
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

# Setting a threshold for feature importance
threshold = 0.001  # You can adjust this value as needed

# Selecting and storing feature names with importances above the given threshold
selected_features = feature_importance_df[feature_importance_df['Importance'] > threshold]['Feature'].tolist()

# Printing out the selected features
print("Selected Features based on Importance:")
print(selected_features)

# Dropping non-selected features from the dataset
non_selected_features = [col for col in df.columns if col not in selected_features]

df.drop(non_selected_features, axis=1, inplace=True)
df

#assigning my dataframe which now contains only my selected features to x
x=df

#scaling x
from sklearn.preprocessing import StandardScaler

# Creating an instance of StandardScaler
scaler = StandardScaler()

# Fitting the scaler to my x and transforming them
scaled = scaler.fit_transform(x)

#saving my scaled model
import pickle
pickle_out=open("scaled.pkl","wb")
pickle.dump(scaled,pickle_out)
pickle_out.close()

#Splitting the dataset into training and testing sets, with 20% of the data reserved for testing, using a fixed random seed for reproducibility.
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(scaled, y, test_size=0.2, random_state=32)

#Checking the shape (dimensions) of the variable X_train
X_train.shape

#Checking the shape(dimensions) of the variable X_test
X_test.shape

from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.model_selection import cross_val_score
import  xgboost as xgb
from sklearn.metrics import mean_absolute_error
#Using the Random Forest
rf=RandomForestRegressor()
rf.fit(X_train,y_train)

#Using the gradientboosting regressor
gb=GradientBoostingRegressor(max_depth=2,n_estimators=3,learning_rate=1.0)
gb.fit(X_train,y_train)

#Using the XGBoost
xgb_regression=xgb.XGBRegressor()
xgb_regression.fit(X_train,y_train)

#Finding the mean score for each model to get an initial sense of how each model is performing before hyperparameter tuning.
rf_score=cross_val_score(rf, X_test, y_test,cv=5)
print("The Random Forest Regressor score is :", rf_score.mean())

gb_score=cross_val_score(gb, X_test, y_test,cv=5)
print("The Gradient Boosting Regressor score is :", gb_score.mean())

xgb_regression_score=cross_val_score(rf, X_test, y_test,cv=5)
print("The XGBoost Regressor score is :", xgb_regression_score.mean())

#Grid search and cross-validation-ensemble-Random forest regressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import KFold
import  xgboost as xgb
from sklearn.metrics import mean_absolute_error

# Initializing the Random Forest Regressor
rf = RandomForestRegressor()

# Defining the hyperparameters and their possible values for the grid search
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20, 30],
}

# Creating a grid search with cross-validation
grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=KFold(n_splits=5, shuffle=True, random_state=32))

# Fitting the grid search to the training data
grid_search.fit(X_train, y_train)

# Getting the best model with the best hyperparameters
best_rf = grid_search.best_estimator_



# Predicting on the test data using the best model
y_pred = best_rf.predict(X_test)
mae = mean_absolute_error(y_test, y_pred)
print("Mean Absolute Error for the Best Random Forest Model:", mae)

# Using the gradientboosting regressor
gb = GradientBoostingRegressor()

# Creating a grid search with cross- validation for gradientBoosting
grid_search_gb = GridSearchCV(estimator=gb, param_grid=param_grid, cv=KFold(n_splits=5, shuffle=True, random_state=32))

# Fitting the grid search to the training data
grid_search_gb.fit(X_train, y_train)

# Getting the best model with the best hyperparameters
best_gb = grid_search_gb.best_estimator_

best_gb.fit(X_train, y_train)  # Fitting the best model to the training data

# Predicting on the test data
y_pred = best_gb.predict(X_test)

# Calculating the Mean Absolute Error (MAE)
mae = mean_absolute_error(y_test, y_pred)
print("Mean Absolute Error for the Best Gradient Boosting Model:", mae)

#Using the XGBoost
xgb_regression = xgb.XGBRegressor()
param = {
    'n_estimators': [100, 200, 300],
    'max_depth': [10, 15, 20] ,
    }


# Creating a grid search with cross-validation for the XGBoost
grid_search_xgb = GridSearchCV(estimator=xgb_regression, param_grid=param, cv=KFold(n_splits=5, shuffle=True, random_state=32))

# Fitting the grid search to the training data
grid_search_xgb.fit(X_train, y_train)

# Getting the best model with the best hyperparameters
best_xgb = grid_search_xgb.best_estimator_

best_xgb.fit(X_train, y_train)  # Fit the best model to the training data

# Predicting on the test data
y_pred = best_xgb.predict(X_test)

# Calculating the Mean Absolute Error (MAE)
mae = mean_absolute_error(y_test, y_pred)
print("Mean Absolute Error for the Best XGB Regressor Model:", mae)

#Getting the best ensemble
from sklearn.ensemble import VotingRegressor
voting_regression = VotingRegressor(estimators=[
    ('RandomForest', best_rf),
    ('GradientBoosting', best_gb),
    ('XGBoost', best_xgb)
])

# Fitting the ensemble to the training data
voting_regression.fit(X_train, y_train)

# Predicting on the test data using the ensemble model
ensemble_predictions = voting_regression.predict(X_test)

# Evaluating the performance of the ensemble model using mean squared error
from sklearn.metrics import mean_squared_error

# Evaluating the performance of the ensemble model using mean absolute error
mae = mean_absolute_error(ensemble_predictions, y_test)
print("Mean Absolute Error for Ensemble:", mae)

#mounting the drive
import pandas as pd
import os
from google.colab import drive
drive.mount('/content/drive')

#loading our data
file_path= os.path.abspath(r"/content/drive/MyDrive/midsem-project/players_22.csv")
data= pd.read_csv(file_path)

#assigning the selected features we got from players_21 to x
selected_features_2 = ['value_eur', 'release_clause_eur', 'age', 'potential', 'movement_reactions', 'wage_eur', 'gk']

x = data[selected_features_2]

#Assigning overall column data to y
y = data['overall']

import pandas as pd

# Identifying the categorical columns within our selected features
categorical_columns = x.select_dtypes(include=['object']).columns

# Identifying the numeric columns within our selected feautures
numeric_columns = x.select_dtypes(exclude=['object']).columns

# Creating separate DataFrames for categorical and numeric data
categorical_data = x[categorical_columns]
numeric_data = x[numeric_columns]

#imputing the numeric and categorical data before the correlation
#Filling missing values in numeric data with the median.
from sklearn.impute import SimpleImputer
imp=SimpleImputer(strategy='median')
a=imp.fit_transform(numeric_data)
numeric_data_impute = pd.DataFrame(a,columns=numeric_columns)

# Filling missing values in categorical_data using forward fill (ffill)
categorical_data_imputed = categorical_data.fillna(method='ffill')

# Making sure the result is a DataFrame with the same column names
categorical_data_imputed = pd.DataFrame(categorical_data_imputed, columns=categorical_data.columns)

#Encoding the categorical data
from sklearn.preprocessing import LabelEncoder

# Initializing the LabelEncoder
label_encoder = LabelEncoder()

# Looping through each column in the imputed categorical data
for col in categorical_data_imputed.columns:
    categorical_data_imputed[col] = label_encoder.fit_transform(categorical_data_imputed[col])

import pandas as pd
# Combining the imputed numeric and label-encoded categorical data into a single DataFrame
x = pd.concat([numeric_data_impute, categorical_data_imputed], axis=1)

#displaying x
x.info()

#scaling my x
from sklearn.preprocessing import StandardScaler

# Creating an instance of StandardScaler
scaler = StandardScaler()

# Fitting the scaler to our selected features and transforming them
scaled = scaler.fit_transform(x)

from sklearn.metrics import mean_absolute_error
from sklearn.ensemble import RandomForestRegressor

#Using the Random Forest Regressor model to test player22
model = RandomForestRegressor(n_estimators=100, random_state=32)  # Adjust hyperparameters as needed

# Fitting the model to the entire dataset
model.fit(a, y)

# Predicting on the entire dataset
y_pred = model.predict(a)

# Calculating the mean absolute error
mae = mean_absolute_error(y, y_pred)
print("Mean Absolute Error:", mae)

# Testing the GradientBoosting Regressor
from sklearn.ensemble import GradientBoostingRegressor
model = GradientBoostingRegressor(n_estimators=100, random_state=32)  # Adjust hyperparameters as needed

# Fitting the model to the entire dataset
model.fit(a, y)

# Predicting on the entire dataset
y_pred = model.predict(a)

# Calculating the mean absolute error
mae = mean_absolute_error(y, y_pred)
print("Mean Absolute Error:", mae)

# Testing the xgtBoosting Regressor
import xgboost as xgb
model =xgb.XGBRegressor(n_estimators=100, random_state=32)  # Adjust hyperparameters as needed

# Fitting the model to the entire dataset
model.fit(a, y)

# Predicting on the entire dataset
y_pred = model.predict(a)

# Calculating the mean absolute error
mae = mean_absolute_error(y, y_pred)
print("Mean Absolute Error:", mae)

#Saving the model
import pickle
pickle_out=open("bestmodel.pkl","wb")
pickle.dump(voting_regression, pickle_out)
pickle_out.close()